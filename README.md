# ðŸ§¾ LedgerX â€“ AI-Powered Invoice Intelligence Platform  
### MLOps: Data Pipeline Submission

---

## ðŸ“˜ Overview
This repository contains the **LedgerX Data Pipeline**, the foundation of our AI-powered invoice-processing MLOps platform.  
The pipeline is fully reproducible, container-ready, and version-controlled using **Git + DVC**, orchestrated with **Airflow**, validated via **Great Expectations**, and tested using **pytest**.

## Team Members
- Jash Bhavesh Shah
- Lochan Enugula
- Samruddhi Bansod
- Rutuja Jadhav
- Nirali Hirpara
- Deep Bhanushali

### Core Features
- ðŸ”„ **Automated data ingestion** from Roboflow datasets  
- ðŸ§© **Preprocessing** â€“ resize, normalization, blur detection, checksum generation  
- âœ… **Validation** â€“ schema + quality checks using Great Expectations  
- ðŸ§± **Data versioning** â€“ tracked with DVC  
- ðŸª¶ **Airflow orchestration** â€“ 8-stage DAG workflow  
- ðŸ§ª **Unit testing** â€“ automated pytest suite  
- âš™ï¸ **CI/CD** â€“ GitHub Actions integration  

---

## ðŸ“Š Dataset Summary
- **Total Documents** : 6279  
- **Train** : 4395 (70 %)  
- **Validation** : 942 (15 %)  
- **Test** : 942 (15 %)

### ðŸ“ˆ Quality Metrics
- **Average Quality Score :** 0.601  
- **Blur Rate :** 1.99 %

### âš™ï¸ Components
- Data ingestion from 2 Roboflow datasets  
- Preprocessing with quality assessment  
- Stratified train/val/test splitting  
- DVC version control  
- Airflow orchestration  

---

## âš™ï¸ Environment Setup

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/Lochan9/ledgerx-mlops.git
cd ledgerx-mlops
```

### 2ï¸âƒ£ Create & Activate Virtual Environment
```bash
python -m venv venv
source venv/bin/activate        # Windows â†’ venv\Scripts\activate
```

### 3ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Configure Roboflow API Key
```bash
export ROBOFLOW_API_KEY="your_api_key_here"
```

---

## ðŸš€ Running the Pipeline

### ðŸ§  Run End-to-End in Python
```bash
python -c "from src.data_pipeline import DataPipeline; pipeline = DataPipeline(); pipeline.run_pipeline()"
```

### ðŸª¶ Run with Airflow
```bash
airflow standalone
```
Then open [http://localhost:8080](http://localhost:8080) â†’ enable **ledgerx_data_pipeline**.  
> DAG flow : check â†’ verify â†’ preprocess â†’ split â†’ validate â†’ test â†’ data card â†’ DVC add  

### ðŸ§ª Run Unit Tests
```bash
pytest -v tests/test_pipeline.py
```

---

## ðŸ—‚ï¸ Repository Structure
```
ledgerx-mlops/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                     # Roboflow datasets (2 sources)
â”‚   â”œâ”€â”€ processed/               # Preprocessed images + metadata
â”‚   â””â”€â”€ splits/                  # Train/Val/Test CSVs
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ data_pipeline.py         # Core LedgerX Data Pipeline class
â”‚
â”œâ”€â”€ airflow_dags/
â”‚   â””â”€â”€ ledgerx_data_pipeline_dag.py   # 8-task Airflow DAG
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_pipeline.py         # Unit tests for pipeline and validation
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup_great_expectations.py    # Schema & data validation
â”‚   â”œâ”€â”€ generate_eda.py                # EDA + profiling reports
â”‚   â”œâ”€â”€ benchmark_performance.py       # Stage benchmarking & throughput
â”‚   â””â”€â”€ validate_pipeline.py           # Automated validation logic
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ pipeline_config.yaml      # Central YAML configuration file
â”‚
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ performance_report.json   # Throughput + system metrics
â”‚   â”œâ”€â”€ data_profile_report.html  # ydata-profiling output
â”‚   â””â”€â”€ eda_visualizations.png    # Generated EDA visuals
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ ARCHITECTURE.md           # System & Git architecture diagrams
â”‚   â”œâ”€â”€ SETUP.md                  # Environment & setup instructions
â”‚   â””â”€â”€ CONTRIBUTING.md           # Developer workflow & code standards
â”‚
â”œâ”€â”€ .github/workflows/ci-cd.yml   # GitHub Actions for CI/CD pipeline
â”œâ”€â”€ .dvc/                         # DVC internal cache & config
â”œâ”€â”€ Dockerfile                    # FastAPI container image definition
â”œâ”€â”€ docker-compose.yml            # API service orchestration
â”œâ”€â”€ requirements.txt              # Python dependencies (pinned)
â”œâ”€â”€ pipeline.log                  # Runtime log of data pipeline
â””â”€â”€ README.md                     # Project overview + execution guide
```

---

## ðŸ” Reproducibility & DVC Tracking

### Add Data to DVC
```bash
dvc add data/processed
dvc add data/splits
git add data/processed.dvc data/splits.dvc .dvc/config .gitignore
git commit -m "Add DVC tracking for data pipeline"
```

### Restore Versioned Data
```bash
dvc pull
```

DVC ensures **immutable, shareable dataset states** so anyone can clone the repo, pull versioned data, and rerun the pipeline identically.

---

## ðŸ§© Validation & Testing
- **Great Expectations** validates schema, column order, value ranges, and duplicates.  
- **Pytest** checks config, metadata, and data-split integrity.  
- **CI/CD** workflow (`.github/workflows/ci-cd.yml`) runs lint + tests + validation on every push.

---

## ðŸ§  Performance Metrics
| Stage | Duration (s) | Records Processed | Throughput (records/s) |
|-------|--------------:|------------------:|-----------------------:|
| Data Ingestion | 45.2 | 6279 | 139.0 |
| Preprocessing | 850.0 | 6279 | 7.38 |
| Splitting | 8.5 | 6279 | 738.7 |
| Validation | 3.2 | 6279 | 1 962.2 |

> **Insight:** Preprocessing (~850 s) is the bottleneck; planned optimization via multiprocessing.

---

## ðŸŽ¯ Reproducibility Summary
- All parameters in `config/pipeline_config.yaml`  
- Deterministic splits (`random_seed = 42`)  
- DVC-tracked datasets  
- CI/CD ensures consistent runs across environments  

---

